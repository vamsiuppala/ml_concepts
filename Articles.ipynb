{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of choice to move from one point ot another in an objective fashion in an attempt to reduce the cost function, so as to find the point at which the cost function is the least, i.e. has a zero gradient. While reaching the true global cost function is not guaranteed (they're possible for convex functions though) - many methods can be used to reach a good low value. Unlike the purely derivate mathematical methodologies like Newton's, Haley's and He-Prabhu's methods of reaching global minimum, these are iterative and numerical.\n",
    "\n",
    "[Sebastian Ruder's overview of gradient descent algorithms](https://github.com/vamsiuppala/ml_concepts.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the pitfalls of not understanding backprop properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backbone of deep learning\n",
    "\n",
    "[Andrej Karpathy's quick read post](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)on how backprop works. It's actualy fairly simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important concept in computer vision. It is basically a filter/kernel (which itself is in a matrix form) that when applied onto an image (which is also a multi dimensional matrix of numbers in reality) results in another multi dimensional matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [An introduction to convolutions](http://www.cs.cornell.edu/courses/cs1114/2013sp/sections/S06_convolution.pdf)\n",
    "The section 3 - Edge detection has some questions/exercises regarding the use of Kernels. Try them out on your own\n",
    "\n",
    "2. [Convolution Arithmetic](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n",
    "\n",
    "3. Basic Intro to CNNs: \n",
    "A [simple](https://brohrer.github.io/how_convolutional_neural_networks_work.html) 5 minute read as to how convolutions work and the simple matrix manipulations involved in Pooling, ReLUs and full connected layers\n",
    "\n",
    "4. What's Image Processing?:\n",
    "A [deep conceptual article](https://openframeworks.cc/ofBook/chapters/image_processing_computer_vision.html) on image processing. Skip it if you aren't looking for some fundamentals of image processing, how images correspond to matrices and the nitty gritty details of image processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization, i.e. methods to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common problem with the traditional statistical approach to modeling is to decrease the number of parameters so as to avoid overfitting. In real life though, that isn't effective, and therefore the more parameters the better to capture the trends/patters. While this may lead to overfitting, the best way to avoid it is through regularization. There are four main ways to do so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Batch Normalization:\n",
    "This is one of the four most important / highly effective methods of avoiding overfitting. Jeremy says simple BN is enough, although [this article](https://arthurdouillard.com/post/normalization/) highlights a few more methods other than the simple batch norm. The authors though argue that sometimes BN elimiates the need for dropout altogether.\n",
    "\n",
    "2. Dropout:\n",
    "3. Weight Decay or L2 Regularization:\n",
    "4. Data Augmentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Log loss, multinomial logistic loss, logistic loss, cross entropy loss: \n",
    "They're all the same - check this [article](https://gombru.github.io/2018/05/23/cross_entropy_loss/) for explanation. It's used in cases of classification. Simple expression = -plog(p)-(1-p)log(1-p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BERT: \n",
    "Interesting article explaining Google's latest [NLP model](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
