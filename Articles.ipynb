{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of choice to move from one point ot another in an objective fashion in an attempt to reduce the cost function, so as to find the point at which the cost function is the least, i.e. has a zero gradient. While reaching the true global cost function is not guaranteed (they're possible for convex functions though) - many methods can be used to reach a good low value. Unlike the purely derivate mathematical methodologies like Newton's, Haley's and He-Prabhu's methods of reaching global minimum, these are iterative and numerical.\n",
    "\n",
    "[Sebastian Ruder's overview of gradient descent algorithms](https://github.com/vamsiuppala/ml_concepts.git)\n",
    "\n",
    "1. SGD\n",
    "2. Adam\n",
    "3. Momentum\n",
    "4. RMS Prop - created by Geoffrey Hinton - didn't even write a paper, it was first mentioned (and thereby the correct citation) was his Coursera MOOC.\n",
    "\n",
    "Pytorch's optim library has all of these implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the pitfalls of not understanding backprop properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backbone of deep learning\n",
    "\n",
    "[Andrej Karpathy's quick read post](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)on how backprop works. It's actualy fairly simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important concept in computer vision. It is basically a filter/kernel (which itself is in a matrix form) that when applied onto an image (which is also a multi dimensional matrix of numbers in reality) results in another multi dimensional matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [An introduction to convolutions](http://www.cs.cornell.edu/courses/cs1114/2013sp/sections/S06_convolution.pdf)\n",
    "The section 3 - Edge detection has some questions/exercises regarding the use of Kernels. Try them out on your own\n",
    "\n",
    "2. [Convolution Arithmetic](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n",
    "\n",
    "3. Basic Intro to CNNs: \n",
    "A [simple](https://brohrer.github.io/how_convolutional_neural_networks_work.html) 5 minute read as to how convolutions work and the simple matrix manipulations involved in Pooling, ReLUs and full connected layers\n",
    "\n",
    "4. What's Image Processing?:\n",
    "A [deep conceptual article](https://openframeworks.cc/ofBook/chapters/image_processing_computer_vision.html) on image processing. Skip it if you aren't looking for some fundamentals of image processing, how images correspond to matrices and the nitty gritty details of image processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization, i.e. methods to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complexity doesn't have to be defined by number of parameters. More parameters is okay with good regularization. More parameters are necessary - it's good in practice as opposed to theory - as established by Neural Networks' success.\n",
    "\n",
    "A common problem with the traditional statistical approach to modeling is to decrease the number of parameters so as to avoid overfitting. In real life though, that isn't effective, and therefore the more parameters the better to capture the trends/patters. While this may lead to overfitting, the best way to avoid it is through regularization. There are four main ways to do so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Batch Normalization:\n",
    "This is one of the four most important / highly effective methods of avoiding overfitting. Jeremy says simple BN is enough, although [this article](https://arthurdouillard.com/post/normalization/) highlights a few more methods other than the simple batch norm. The authors though argue that sometimes BN elimiates the need for dropout altogether.\n",
    "\n",
    "2. Dropout:\n",
    "3. Weight Decay or L2 Regularization: Too much weight decay and no matter how much you train, you never fit enough. Too little weight decay, you overfit kind of early and have to stop early. 0.1 is a good number. fastai's default is 0.01 because they want to be a bit conservative for those rare edge cases, but one can always pass a wd=1e-1 argument while creating learners.\n",
    "\n",
    "4. Data Augmentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Log loss, multinomial logistic loss, logistic loss, cross entropy loss: \n",
    "They're all the same - check this [article](https://gombru.github.io/2018/05/23/cross_entropy_loss/) for explanation. It's used in cases of classification. Simple expression = -plog(p)-(1-p)log(1-p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BERT: \n",
    "Interesting article explaining Google's latest [NLP model](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminative Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving different learning rates for backpropagating throughout different parts of the neural network. It is not famous, or widely used, but it helps in transfer learning, by training the initial parts of the neural network that identifies edges etc. almost the same (because they don't really need to be changed that much), and focusing more on training the final parts of the neural network that are mostly fine tuned for the specific task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the initial layers, except the ones created at the end of a neural network (randomly generated added layers that replaces imagenet classes with the classes that we have to train for - for image classification say) - this ensures that the final layer is trained first.\n",
    "The network can then be unfrozen and the entire network can be retrained using discriminative learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast AI syntax:\n",
    "1. fit (1, 1e-3) - every layer gets 1e-3 without discriminative learning\n",
    "2. fit (1, slice(1e-3)) - last layer gets 1e-3, rest 1e-3/3\n",
    "3. fit (1, slice(1e-5, 1e-3) - first \"layer group gets\" 1e-5, last \"layer\" gets 1e-3, \"layer group\" in between get multiplicatively equally divided between 1e-5 and 1e-3. So for resnet-34, the first layer group gets 1e-5, the middle layer group gets 1e-4 and the last layer gets 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An epoch looks at the entire input set once. One epoch means you've looked at every example once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look something up in an array - that's all it is. These are called factors in general, and not embedding size. Factors is basically the width of the embedding matrix. For collaborative filtering, 40 is a good number to start on the embedding matrix size, i.e. the n_factors = 40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses Bias and Latent factors (embeddings) for a user and and an item (dot products the latent factors for user and item and adds user bias and item bias) to get to the rating for the user-item combination.\n",
    "\n",
    "Can use bias, factors, weight decay, y_range (range can be set to be slightly higher than the actual range) etc. to fine-tune the collaborative filtering network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
