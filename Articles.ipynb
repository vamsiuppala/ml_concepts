{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of choice to move from one point ot another in an objective fashion in an attempt to reduce the cost function, so as to find the point at which the cost function is the least, i.e. has a zero gradient. While reaching the true global cost function is not guaranteed (they're possible for convex functions though) - many methods can be used to reach a good low value. Unlike the purely derivate mathematical methodologies like Newton's, Haley's and He-Prabhu's methods of reaching global minimum, these are iterative and numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sebastian Ruder's overview of gradient descent algorithms](https://github.com/vamsiuppala/ml_concepts.git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
